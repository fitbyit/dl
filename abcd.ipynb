{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfAjrzZnfkeZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DZXVV4HkZ_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-w3fKPE5kaDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nq9-ufnkaIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiF1Cf46kaL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_0SySTskaPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "brlkGRQ9kaSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5dVLi11ftUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drZYnopUftXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Om2zRFJjftaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ml_0HZMBftc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "feA0GmOXftf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "No1B6WGHftih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Jxfce0gftlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "odoDMMFaftoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azn4ESodftq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IuxOLWfSftt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7kH7Vd_ftw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjxL8MmkftzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4_AQWXGft2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzrsHo8tft5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ToLNkAjft8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfR3EDZ3ft_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLa9hl75fuB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTDA3kc7fuE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_zVPMIshfuH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eH6vfp3fuK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xt-7cOkdfuN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "skXwKugufuQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN"
      ],
      "metadata": {
        "id": "whSo-3m2fvGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.datasets import imdb\n",
        "from random import randint\n",
        "\n",
        "# Load the IMDB dataset, limiting to the top 5000 most frequent words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=80)\n",
        "\n",
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 128))          # Embedding layer for word embeddings\n",
        "model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid'))  # LSTM layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test), shuffle=True, verbose=1)\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Randomly select a review from the test set for evaluation\n",
        "arr_ind = randint(0, len(x_test) - 1)  # Random index\n",
        "index = imdb.get_word_index()           # Word index dictionary\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])  # Reverse dictionary for decoding\n",
        "decoded = \" \".join([reverse_index.get(i - 3, \"#\") for i in x_test[arr_ind]])  # Decode review text\n",
        "\n",
        "# Determine the sentiment of the review\n",
        "predicted_sentiment = \"Positive\" if predictions[arr_ind] >= 0.5 else \"Negative\"\n",
        "actual_sentiment = \"Positive\" if y_test[arr_ind] == 1 else \"Negative\"\n",
        "\n",
        "# Print results\n",
        "print(\"Sentence:\", decoded)\n",
        "print(\"Review:\", predicted_sentiment)\n",
        "print(\"Predicted Value:\", predictions[arr_ind][0])\n",
        "print(\"Expected Value:\", y_test[arr_ind])\n",
        "\n"
      ],
      "metadata": {
        "id": "c-ilQCcIfwfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AUTOENCODER"
      ],
      "metadata": {
        "id": "kaw5Scl5fzNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = layers.Dense(128, activation='sigmoid')(input_img)\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=5,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "encoded_imgs = autoencoder.predict(x_test)\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(encoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "### ADDING SPARISTY\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "encoding_dim = 32\n",
        "input_img = keras.Input(shape=(784,))\n",
        "# Add a Dense layer with a L1 activity regularizer\n",
        "encoded = layers.Dense(encoding_dim, activation='relu',\n",
        "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = keras.Model(input_img, decoded)\n",
        "\n",
        "# This model maps an input to its encoded representation\n",
        "encoder = keras.Model(input_img, encoded)\n",
        "\n",
        "# This is our encoded (32-dimensional) input\n",
        "encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "# Retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# Create the decoder model\n",
        "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=20,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "# Encode and decode some digits\n",
        "# Note that we take them from the *test* set\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)\n",
        "\n",
        "# Use Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## DENOISING\n",
        "#Denoising\n",
        "\n",
        "from tensorflow.keras.layers import GaussianNoise\n",
        "\n",
        "(x_train,_),(x_test,_) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')/255.0\n",
        "x_test = x_test.astype('float32')/255.0\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "x_train=x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))\n",
        "x_test=x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "\n",
        "#Architechture\n",
        "\n",
        "input_img = Input(shape=(784,))\n",
        "#add noise to input\n",
        "noisy_input = GaussianNoise(0.5)(input_img)\n",
        "\n",
        "encoded = Dense(128,activation='sigmoid')(input_img)\n",
        "decoded = Dense(784,activation='sigmoid')(encoded)\n",
        "\n",
        "#Build autencoder model\n",
        "\n",
        "autoencoder = Model(input_img,decoded)\n",
        "#compile\n",
        "autoencoder.compile(optimizer= Adam(),loss='binary_crossentropy')\n",
        "#train\n",
        "autoencoder.fit(x_train,x_train,epochs=5,batch_size=256,shuffle=True,validation_data=(x_test,x_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#encode and decode some digits\n",
        "encoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "# Generate noisy images\n",
        "noise_factor = 0.5\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display noisy\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
        "    plt.imshow(encoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eJgtzNWff2ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OPTIMIZERS"
      ],
      "metadata": {
        "id": "J9utNPMEf4p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "x = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * x + np.random.randn(100, 1)\n",
        "\n",
        "X_b = np.c_[np.ones((100, 1)), x]  # Add bias term\n",
        "\n",
        "\n",
        "\n",
        "# Initialization for all methods\n",
        "theta_sgd = np.random.randn(2, 1)\n",
        "theta_batch = np.random.randn(2, 1)\n",
        "theta_mini = np.random.randn(2, 1)\n",
        "eta = 0.1\n",
        "n_iterations = 10000\n",
        "m = 100\n",
        "batch_size = 20\n",
        "# Stochastic Gradient Descent\n",
        "def stochastic_gradient_descent(X_b, y, theta, eta, n_iterations, m):\n",
        "  for iteration in range(n_iterations):\n",
        "    for i in range(m):\n",
        "      random_index = np.random.randint(m)\n",
        "      xi = X_b[random_index:random_index + 1]\n",
        "      yi = y[random_index:random_index + 1]\n",
        "      gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "      theta = theta - eta * gradients\n",
        "  return theta\n",
        "# Batch Gradient Descent\n",
        "def batch_gradient_descent(X_b, y, theta, eta, n_iterations):\n",
        "  for iteration in range(n_iterations):\n",
        "    gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "  return theta\n",
        "# Mini-Batch Gradient Descent\n",
        "def mini_batch_gradient_descent(X_b, y, theta, eta, n_iterations, m, batch_size):\n",
        "  for iteration in range(n_iterations):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, batch_size):\n",
        "      xi = X_b_shuffled[i:i+batch_size]\n",
        "      yi = y_shuffled[i:i+batch_size]\n",
        "      gradients = 2 / batch_size * xi.T.dot(xi.dot(theta) - yi)\n",
        "      theta = theta - eta * gradients\n",
        "  return theta\n",
        "# Run the optimization functions\n",
        "theta_sgd = stochastic_gradient_descent(X_b, y, theta_sgd, eta, n_iterations, m)\n",
        "theta_batch = batch_gradient_descent(X_b, y, theta_batch, eta, n_iterations)\n",
        "theta_mini = mini_batch_gradient_descent(X_b, y, theta_mini, eta, n_iterations, m, batch_size)\n",
        "# Print results\n",
        "print(\"Stochastic Gradient Descent:\", theta_sgd.flatten())\n",
        "print(\"Batch Gradient Descent:\", theta_batch.flatten())\n",
        "print(\"Mini-Batch Gradient Descent:\", theta_mini.flatten())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(x, y, color=\"grey\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.plot(x, X_b.dot(theta_sgd), label='Stochastic')\n",
        "plt.plot(x, X_b.dot(theta_batch), label='Batch')\n",
        "plt.plot(x, X_b.dot(theta_mini), label='Mini-Batch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Implementing Adavance Optimizers\n",
        "\n",
        "\n",
        "#Momentum\n",
        "#Adagrad\n",
        "#RMSprop\n",
        "\n",
        "theta_momentum = np.random.randn(2, 1)\n",
        "theta_adagrad = np.random.randn(2, 1)\n",
        "theta_rmsprop = np.random.randn(2, 1)\n",
        "eta = 0.1\n",
        "n_iterations = 10000\n",
        "m = 100\n",
        "\n",
        "# Momentum-based Gradient Descent\n",
        "def momentum_gradient_descent(X_b, y, theta, eta, n_iterations, m, beta=0.9):\n",
        "  velocity = np.zeros_like(theta)\n",
        "  for iteration in range(n_iterations):\n",
        "    gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    velocity = beta * velocity + (1 - beta) * gradients\n",
        "    theta = theta - eta * velocity\n",
        "  return theta\n",
        "\n",
        "# Adagrad\n",
        "def adagrad(X_b, y, theta, eta, n_iterations, m, epsilon=1e-7):\n",
        "  s = np.zeros_like(theta)\n",
        "  for iteration in range(n_iterations):\n",
        "    gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    s += gradients ** 2\n",
        "    theta = theta - eta * gradients / (np.sqrt(s) + epsilon)\n",
        "  return theta\n",
        "\n",
        "# RMSprop\n",
        "def rmsprop(X_b, y, theta, eta, n_iterations, m, beta=0.9, epsilon=1e-7):\n",
        "  s = np.zeros_like(theta)\n",
        "  for iteration in range(n_iterations):\n",
        "    gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    s = beta * s + (1 - beta) * gradients ** 2\n",
        "    theta = theta - eta * gradients / (np.sqrt(s) + epsilon)\n",
        "  return theta\n",
        "\n",
        "theta_momentum = momentum_gradient_descent(X_b, y, theta_momentum, eta, n_iterations, m)\n",
        "theta_adagrad = adagrad(X_b, y, theta_adagrad, eta, n_iterations, m)\n",
        "theta_rmsprop = rmsprop(X_b, y, theta_rmsprop, eta, n_iterations, m)\n",
        "\n",
        "# Print results\n",
        "print(\"Momentum-based Gradient Descent:\", theta_momentum.flatten())\n",
        "print(\"Adagrad:\", theta_adagrad.flatten())\n",
        "print(\"RMSprop:\", theta_rmsprop.flatten())\n",
        "\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(x, y, color=\"grey\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.plot(x, X_b.dot(theta_momentum), label='Momentum')\n",
        "plt.plot(x, X_b.dot(theta_adagrad), label='Adagrad')\n",
        "plt.plot(x, X_b.dot(theta_rmsprop), label='RMSprop')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hAbaWLZwf7ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN USING FILTERS"
      ],
      "metadata": {
        "id": "pHWm0l6Rf99K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imshow, imread\n",
        "from skimage.color import rgb2yuv, rgb2hsv, rgb2gray, yuv2rgb, hsv2rgb\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "#Reading Image\n",
        "dog = imread('/content/img.jpg')\n",
        "plt.figure(num=None, figsize=(8,6), dpi=80)\n",
        "imshow(dog)\n",
        "\n",
        "\n",
        "#Filter MAtrices\n",
        "\n",
        "sharpen = np.array([[0,-1,0],\n",
        "                    [-1,5,-1],\n",
        "                    [0,-1,0]])\n",
        "\n",
        "blur = np.array([[0.11,0.11,0.11],\n",
        "                    [0.11,0.11,0.11],\n",
        "                    [0.11,0.11,0.11]])\n",
        "\n",
        "\n",
        "vertical = np.array([[-1,0,1],\n",
        "                    [-2,0,2],\n",
        "                    [-1,0,1]])\n",
        "\n",
        "\n",
        "gaussian = (1/16.0) * np.array([[1,2,1],\n",
        "                                [2,4,2],\n",
        "                                [1,2,1]])\n",
        "\n",
        "\n",
        "\n",
        "#plotting the filters\n",
        "\n",
        "fig,ax = plt.subplots(1,3, figsize = (17,10))\n",
        "ax[0].imshow(sharpen, cmap='gray')\n",
        "ax[0].set_title(f'Sharpen', fontsize=18)\n",
        "\n",
        "ax[1].imshow(blur, cmap='gray')\n",
        "ax[1].set_title(f'Blur', fontsize=18)\n",
        "\n",
        "ax[2].imshow(vertical, cmap='gray')\n",
        "ax[2].set_title(f'Vertical', fontsize=18)\n",
        "\n",
        "\n",
        "#Grayscaling Image\n",
        "dog_gray = rgb2gray(dog)\n",
        "plt.figure(num=None, figsize=(8,6), dpi=80)\n",
        "imshow(dog_gray)\n",
        "\n",
        "\n",
        "#Function for applying filters\n",
        "def multi_convolver(image, kernel, iterations):\n",
        "  for i in range(iterations):\n",
        "    image = convolve2d(image, kernel, 'same', boundary = 'fill', fillvalue = 0)\n",
        "  return image\n",
        "\n",
        "convolved_image = multi_convolver(dog_gray, sharpen, 1)\n",
        "\n",
        "plt.figure(num=None, figsize=(8,6), dpi=80)\n",
        "imshow(convolved_image);\n",
        "\n",
        "\n",
        "\n",
        "#For colored Image\n",
        "def convolver_rgb(image, kernel, iterations = 1):\n",
        "  convolved_image_r = multi_convolver(image[:,:,0], kernel, iterations)\n",
        "  convolved_image_g = multi_convolver(image[:,:,1], kernel, iterations)\n",
        "  convolved_image_b = multi_convolver(image[:,:,2], kernel, iterations)\n",
        "\n",
        "  reformed_image = np.dstack((np.rint(abs(convolved_image_r)),np.rint(abs(convolved_image_g)),np.rint(abs(convolved_image_b))))/255\n",
        "\n",
        "  fig,ax = plt.subplots(1,3, figsize = (17,10))\n",
        "\n",
        "  ax[0].imshow(abs(convolved_image_r), cmap='Reds')\n",
        "  ax[0].set_title(f'Red', fontsize=15)\n",
        "\n",
        "  ax[1].imshow(abs(convolved_image_g), cmap='Greens')\n",
        "  ax[1].set_title(f'Green', fontsize=18)\n",
        "\n",
        "  ax[2].imshow(abs(convolved_image_b), cmap='Blues')\n",
        "  ax[2].set_title(f'Blue', fontsize=18)\n",
        "\n",
        "  return np.array(reformed_image*255).astype(np.uint8)\n",
        "\n",
        "#Can add different filters (defined above) here\n",
        "convolved_rgb_gauss = convolver_rgb(dog, vertical.T ,1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#CNN PARAMETER CALCULATION\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, input_shape=(28,28,3),\n",
        "                 kernel_size = (5,5),\n",
        "                 padding='same',\n",
        "                 use_bias=False))\n",
        "model.add(Conv2D(17, (3,3), padding='same', use_bias=False))\n",
        "model.add(Conv2D(13, (3,3), padding='same', use_bias=False))\n",
        "model.add(Conv2D(7, (3,3), padding='same', use_bias=False))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, input_shape=(28,28,3),\n",
        "                 kernel_size = (5,5),\n",
        "                 padding='same',\n",
        "                 use_bias=True))\n",
        "model.add(Conv2D(17, (3,3), padding='same', use_bias=True))\n",
        "model.add(Conv2D(13, (3,3), padding='same', use_bias=True))\n",
        "model.add(Conv2D(7, (3,3), padding='same', use_bias=True))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, input_shape=(28,28,3),\n",
        "                 kernel_size = (5,5),\n",
        "                 use_bias=False))\n",
        "model.add(Conv2D(17, (3,3), use_bias=False))\n",
        "model.add(Conv2D(13, (3,3), use_bias=False))\n",
        "model.add(Conv2D(7, (3,3), use_bias=False))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, input_shape=(28,28,3),\n",
        "                 kernel_size = (5,5),\n",
        "                 padding='valid',\n",
        "                 use_bias=False))\n",
        "model.add(Conv2D(17, (3,3), padding='valid', use_bias=False))\n",
        "model.add(Conv2D(13, (3,3), padding='valid', use_bias=False))\n",
        "model.add(Conv2D(7, (3,3), padding='valid', use_bias=False))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(10, input_shape=(28,28,3),\n",
        "                 kernel_size = (5,5),\n",
        "                 strides = (1,1),\n",
        "                 padding='valid',\n",
        "                 use_bias=False))\n",
        "model.add(Conv2D(20, (5,5), (2,2), padding='valid', use_bias=False))\n",
        "model.add(Conv2D(40, (5,5), (2,2), padding='valid', use_bias=False))\n",
        "\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(10, input_shape=(39,39,3),\n",
        "                 kernel_size = (3,3),\n",
        "                 padding='valid',))\n",
        "model.add(Conv2D(20, (5,5), (2,2), padding='valid' ))\n",
        "model.add(Conv2D(40, (5,5), (2,2),padding='valid'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# MAX POLLING\n",
        "import tensorflow as tf\n",
        "x = tf.constant([[1., 2., 3.],\n",
        "                 [4., 5., 6.],\n",
        "                 [7., 8., 9.]])\n",
        "x = tf.reshape(x, [1, 3, 3, 1])\n",
        "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid')\n",
        "max_pool_2d(x)\n",
        "\n",
        "\n",
        "\n",
        "x = tf.constant([[1., 2., 3., 4.],\n",
        "                 [5., 6., 7., 8.],\n",
        "                 [9., 10., 11., 12.]])\n",
        "x = tf.reshape(x, [1, 3, 4, 1])\n",
        "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
        "   strides=(2, 2), padding='valid')\n",
        "max_pool_2d(x)\n",
        "\n",
        "\n",
        "x = tf.constant([[1., 2., 3., 4.],\n",
        "                 [5., 6., 7., 8.],\n",
        "                 [9., 10., 11., 12.]])\n",
        "x = tf.reshape(x, [1, 3, 4, 1])\n",
        "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
        "   strides=(2, 2), padding='same')\n",
        "max_pool_2d(x)\n",
        "\n",
        "\n",
        "x = tf.constant([[1., 2., 3., 4.],\n",
        "                 [5., 6., 7., 8.],\n",
        "                 [9., 10., 11., 12.]])\n",
        "x = tf.reshape(x, [1, 3, 4, 1])\n",
        "max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n",
        "   strides=(1, 1), padding='same')\n",
        "max_pool_2d(x)"
      ],
      "metadata": {
        "id": "6Fd37JWpgBW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REGULARIZATION"
      ],
      "metadata": {
        "id": "webf_VoNgDTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from matplotlib.colors import ListedColormap\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_circles,make_moons\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(800)\n",
        "X,y=make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "zero_one_colourmap=ListedColormap(('red','green'))\n",
        "rcParams['figure.figsize']=14,7\n",
        "plt.scatter(X[:,0],X[:,1],\n",
        "c=y, s=100,\n",
        "cmap=zero_one_colourmap)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=1000, verbose=0)\n",
        "\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "reg_model = Sequential()\n",
        "reg_model.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer='l2'))\n",
        "reg_model.add(Dense(1, activation='sigmoid', kernel_regularizer='l2'))\n",
        "reg_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "reg_history = reg_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, verbose=0)\n",
        "\n",
        "plt.plot(reg_history.history['loss'], label='train')\n",
        "plt.plot(reg_history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nc00_CEqgGp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AUGUMENTATION"
      ],
      "metadata": {
        "id": "P_dNzvEugIBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255  # Reshape and normalize\n",
        "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Data Augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(28, 28, 1)),  # Specify input shape here\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "# Create a simple CNN model\n",
        "def create_cnn_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(data_augmentation)  # Include data augmentation in the model\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # Output layer\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_cnn_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='test loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XRu4a1-DgMoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DROP OUT"
      ],
      "metadata": {
        "id": "ouAOgMtNgOwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DropOut with CNN on Mnist dataset\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build CNN model with Dropout\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout layer with 50% rate\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Plot training & validation accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rR2LnRXmgTBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MINIST DATASET FILTER CNN"
      ],
      "metadata": {
        "id": "1qwuDNYlgVab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter on Mnist using CNN\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255  # Reshape and normalize\n",
        "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model():\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolutional layer with 3x3 filters (Sharpening Filter)\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "\n",
        "    # Convolutional layer with 5x5 filters (Blur Filter)\n",
        "    model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
        "\n",
        "    # Pooling layer\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Another convolutional layer (Edge Detection Filter)\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "    # Flattening layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))  # Dropout for regularization\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # Output layer\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_cnn_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='test loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w056GNOugRyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BINARY CLASSIFICATION"
      ],
      "metadata": {
        "id": "UyE_vZE0gbQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Class\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "def mean_squared_error_loss(y_true, y_pred):\n",
        "  return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "## input\n",
        "input_data = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "output_data = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "## Traning Parameter\n",
        "np.random.seed(42)\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "## Random Weights & Bias\n",
        "weights_input_to_hidden = np.random.rand(input_size, hidden_size)\n",
        "bias_hidden = np.random.rand(hidden_size)\n",
        "\n",
        "weights_hidden_to_output = np.random.rand(hidden_size, output_size)\n",
        "bias_output = np.random.rand(output_size)\n",
        "\n",
        "#Training Parameter\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #forward pass\n",
        "  hidden_input = np.dot(input_data, weights_input_to_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "  final_input = np.dot(hidden_output, weights_hidden_to_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "\n",
        "\n",
        "  #computing loss\n",
        "  loss = mean_squared_error_loss(output_data, final_output)\n",
        "\n",
        "  #backpropagation\n",
        "  error = final_output-output_data\n",
        "  gradient_output = error*sigmoid_derivative(final_output)\n",
        "\n",
        "  # hidden layer eror and gradient\n",
        "  error_hidden = gradient_output.dot(weights_hidden_to_output.T)\n",
        "  gradient_hidden = error_hidden*sigmoid_derivative(hidden_output)\n",
        "\n",
        "  #updatingweight and biases\n",
        "  weights_hidden_to_output-=learning_rate*np.dot(hidden_output.T, gradient_output)\n",
        "  bias_output-=learning_rate*np.mean(gradient_output, axis=0)\n",
        "\n",
        "  weights_input_to_hidden-=learning_rate*np.dot(input_data.T, gradient_hidden)\n",
        "  bias_hidden-=learning_rate*np.mean(gradient_hidden, axis=0)\n",
        "\n",
        "  #print 1000 loss\n",
        "  if epoch%1000==0:\n",
        "    print(f'Epoch: {epoch}, Loss: {loss}')\n",
        "\n",
        "#compute out for each input pair after training\n",
        "result = []\n",
        "for input_pair in input_data:\n",
        "  hidden_input = np.dot(input_pair, weights_input_to_hidden)+bias_hidden\n",
        "  hidden_output = sigmoid(hidden_input)\n",
        "  final_input = np.dot(hidden_output, weights_hidden_to_output)+bias_output\n",
        "  final_output = sigmoid(final_input)\n",
        "  result.append(final_output)\n",
        "  print(f'Input: {input_pair}, Output: {final_output}')\n",
        "\n"
      ],
      "metadata": {
        "id": "RRd2WilkghzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MULTICLASSIFICATION"
      ],
      "metadata": {
        "id": "bpWVa8SLgjN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "\n",
        "def categorical_crossentropy_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "# Input data and output data\n",
        "input_data = np.array([[0.8, 0.6, 0.7]])\n",
        "output_data = np.array([[0, 1, 0]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "weights_input_to_hidden = np.array([[0.2, 0.4, 0.1],\n",
        "                                    [0.5, 0.3, 0.2],\n",
        "                                    [0.3, 0.7, 0.8]])\n",
        "\n",
        "bias_hidden = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "weights_hidden_to_output = np.array([[0.6, 0.4, 0.5],\n",
        "                                     [0.1, 0.2, 0.3],\n",
        "                                     [0.3, 0.7, 0.2]])\n",
        "\n",
        "bias_output = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward Pass\n",
        "    hidden_input = np.dot(input_data, weights_input_to_hidden) + bias_hidden\n",
        "    hidden_output = relu(hidden_input)\n",
        "\n",
        "    final_input = np.dot(hidden_output, weights_hidden_to_output) + bias_output\n",
        "    final_output = softmax(final_input)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = categorical_crossentropy_loss(output_data, final_output)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss:.4f}\")\n",
        "\n",
        "    # Backpropagation\n",
        "    error = final_output - output_data\n",
        "\n",
        "    # Gradient for weights_hidden_to_output and bias_output\n",
        "    gradient_weights_hidden_to_output = np.dot(hidden_output.T, error)\n",
        "    gradient_bias_output = np.sum(error, axis=0, keepdims=True)\n",
        "\n",
        "    # Propagate the error back to the hidden layer\n",
        "    error_hidden = np.dot(error, weights_hidden_to_output.T)\n",
        "    error_hidden *= relu_derivative(hidden_input)\n",
        "\n",
        "    # Gradient for weights_input_to_hidden and bias_hidden\n",
        "    gradient_weights_input_to_hidden = np.dot(input_data.T, error_hidden)\n",
        "    gradient_bias_hidden = np.sum(error_hidden, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases using gradient descent\n",
        "    weights_hidden_to_output -= learning_rate * gradient_weights_hidden_to_output\n",
        "    bias_output -= learning_rate * gradient_bias_output.squeeze()\n",
        "    weights_input_to_hidden -= learning_rate * gradient_weights_input_to_hidden\n",
        "    bias_hidden -= learning_rate * gradient_bias_hidden.squeeze()\n",
        "\n",
        "# Print final loss after training\n",
        "print(f\"Final Loss: {loss:.4f}\")\n",
        "print(f\"Input: {input_data}, Output : {final_output}\")"
      ],
      "metadata": {
        "id": "3wLxb_zVgl7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#brain tumor detection\n"
      ],
      "metadata": {
        "id": "qc-heLmTgnWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit([[0], [1]])\n",
        "\n",
        "# 0 - Tumor\n",
        "# 1 - Normal\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "zip_train=zipfile.ZipFile('/content/drive/MyDrive/brain_tumor_dataset.zip', 'r')\n",
        "zip_train.extractall('/content/drive/MyDrive')\n",
        "zip_train.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# This cell updates result list for images with tumor\n",
        "data = []\n",
        "paths = []\n",
        "result = []\n",
        "\n",
        "for r, d, f in os.walk(r'/content/drive/MyDrive/brain_tumor_dataset/yes'):\n",
        "    for file in f:\n",
        "        if '.jpg' in file:\n",
        "            paths.append(os.path.join(r, file))\n",
        "\n",
        "for path in paths:\n",
        "    img = Image.open(path)\n",
        "    img = img.resize((128,128))\n",
        "    img = np.array(img)\n",
        "    if(img.shape == (128,128,3)):\n",
        "        data.append(np.array(img))\n",
        "        result.append(encoder.transform([[0]]).toarray())\n",
        "\n",
        "\n",
        "# This cell updates result list for images with tumor\n",
        "\n",
        "paths = []\n",
        "for r, d, f in os.walk(r\"/content/drive/MyDrive/brain_tumor_dataset/no\"):\n",
        "    for file in f:\n",
        "        if '.jpg' in file:\n",
        "            paths.append(os.path.join(r, file))\n",
        "\n",
        "for path in paths:\n",
        "    img = Image.open(path)\n",
        "    img = img.resize((128,128))\n",
        "    img = np.array(img)\n",
        "    if(img.shape == (128,128,3)):\n",
        "        data.append(np.array(img))\n",
        "        result.append(encoder.transform([[1]]).toarray())\n",
        "\n",
        "\n",
        "\n",
        "data = np.array(data)\n",
        "data.shape\n",
        "\n",
        "\n",
        "result = np.array(result)\n",
        "result = result.reshape(139,2)\n",
        "\n",
        "\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(data, result, test_size=0.2, shuffle=True, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(2, 2), input_shape=(128, 128, 3), padding = 'Same'))\n",
        "model.add(Conv2D(32, kernel_size=(2, 2),  activation ='relu', padding = 'Same'))\n",
        "\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))\n",
        "model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer='Adamax')\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "y_train.shape\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 30, batch_size = 40, verbose = 1,validation_data = (x_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Test', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "def names(number):\n",
        "    if number==0:\n",
        "        return 'Its a Tumor'\n",
        "    else:\n",
        "        return 'No, Its not a tumor'\n",
        "\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "img = Image.open(r\"/content/drive/MyDrive/brain_tumor_dataset/no/43 no.jpg\")\n",
        "x = np.array(img.resize((128,128)))\n",
        "x = x.reshape(1,128,128,3)\n",
        "res = model.predict_on_batch(x)\n",
        "classification = np.where(res == np.amax(res))[1][0]\n",
        "imshow(img)\n",
        "print(str(res[0][classification]*100) + '% Confidence This Is ' + names(classification))\n"
      ],
      "metadata": {
        "id": "zCvqv6NzgqYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cat dog classification"
      ],
      "metadata": {
        "id": "c4G-H3_djlO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,BatchNormalization,Dropout\n",
        "\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!kaggle datasets download -d salader/dogs-vs-cats\n",
        "\n",
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()\n",
        "\n",
        "\n",
        "#generators are very useful to process larger amount of data it divide data into serveral batches\n",
        "\n",
        "train_ds=tf.keras.utils.image_dataset_from_directory(\n",
        "    directory = '/content/train',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'int',\n",
        "    batch_size = 32,\n",
        "    image_size = (256,256)\n",
        ")\n",
        "\n",
        "validation_ds=tf.keras.utils.image_dataset_from_directory(\n",
        "    directory = '/content/test',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'int',\n",
        "    batch_size = 32,\n",
        "    image_size = (256,256)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#Normalize the data\n",
        "def process(image,label):\n",
        "  image = tf.cast(image/255. ,tf.float32)\n",
        "  return image,label\n",
        "\n",
        "train_ds = train_ds.map(process)\n",
        "validation_ds = validation_ds.map(process)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model_1 = Sequential()\n",
        "\n",
        "# Convolutional Layer 1\n",
        "model_1.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu',\n",
        "                   input_shape=(256,256,3),\n",
        "                   kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "# Convolutional Layer 2\n",
        "model_1.add(Conv2D(64, kernel_size=(3,3), padding='valid', activation='relu',\n",
        "                   kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "# Convolutional Layer 3\n",
        "model_1.add(Conv2D(128, kernel_size=(3,3), padding='valid', activation='relu',\n",
        "                    kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "# Flatten Layer\n",
        "model_1.add(Flatten())\n",
        "\n",
        "# Dense Layer 1\n",
        "model_1.add(Dense(128, activation='relu',\n",
        "                   kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
        "model_1.add(Dropout(0.1))\n",
        "\n",
        "# Dense Layer 2\n",
        "model_1.add(Dense(64, activation='relu',\n",
        "                   kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
        "model_1.add(Dropout(0.1))\n",
        "\n",
        "# Output Layer\n",
        "model_1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "model_1.summary()\n",
        "\n",
        "model_1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history=model_1.fit(train_ds,epochs=30,validation_data=validation_ds)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'],color='red',label='train')\n",
        "plt.plot(history.history['val_accuracy'],color='blue',label='validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'],color='red',label='train')\n",
        "plt.plot(history.history['val_loss'],color='blue',label='validation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "import cv2\n",
        "test_dog=cv2.imread('/content/sample_data/pexels-svetozar-milashevich-99573-1490908.jpg')\n",
        "plt.imshow(test_dog)\n",
        "\n",
        "test_dog.shape\n",
        "test_dog=cv2.resize(test_dog,(256,256))\n",
        "\n",
        "test_input=test_dog.reshape((1,256,256,3))\n",
        "model_1.predict(test_input)\n",
        "\n",
        "test_cat=cv2.imread('/content/sample_data/cat-1192026_640.jpg')\n",
        "\n",
        "plt.imshow(test_cat)\n",
        "test_cat.shape\n",
        "\n",
        "test_cat=cv2.resize(test_cat,(256,256))\n",
        "test_inputt=test_cat.reshape((1,256,256,3))\n",
        "model_1.predict(test_inputt)\n",
        "\n",
        "\n",
        "# Load and preprocess the image\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.cast(img_array / 255.0, tf.float32)  # Normalize to [0, 1]\n",
        "    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
        "    return img_array\n",
        "img_array = load_and_preprocess_image('/content/sample_data/cat-1192026_640.jpg')\n",
        "\n",
        "# Make a prediction\n",
        "predictions = model_1.predict(img_array)\n",
        "\n",
        "# Output the prediction\n",
        "class_prediction = (predictions > 0.5).astype(\"int32\")\n",
        "print(\"Predicted class:\", \"Dog\" if class_prediction[0][0] == 1 else \"Cat\")\n",
        "\n",
        "\n",
        "# Load and preprocess the image\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.cast(img_array / 255.0, tf.float32)  # Normalize to [0, 1]\n",
        "    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
        "    return img_array\n",
        "img_array = load_and_preprocess_image('/content/sample_data/pexels-svetozar-milashevich-99573-1490908.jpg')\n",
        "\n",
        "# Make a prediction\n",
        "predictions = model_1.predict(img_array)\n",
        "\n",
        "# Output the prediction\n",
        "class_prediction = (predictions > 0.5).astype(\"int32\")\n",
        "print(\"Predicted class:\", \"Dog\" if class_prediction[0][0] == 1 else \"Cat\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.cast(img_array / 255.0, tf.float32)  # Normalize to [0, 1]\n",
        "    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
        "    return img, img_array\n",
        "\n",
        "# Path to the image\n",
        "image_path = '/content/sample_data/cat-1192026_640.jpg'\n",
        "img, img_array = load_and_preprocess_image(image_path)\n",
        "\n",
        "# Make a prediction\n",
        "predictions = model_1.predict(img_array)\n",
        "\n",
        "# Output the prediction\n",
        "class_prediction = (predictions > 0.5).astype(\"int32\")\n",
        "predicted_class = \"Dog\" if class_prediction[0][0] == 1 else \"Cat\"\n",
        "\n",
        "# Display the image and prediction\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Prediction: {predicted_class}\")\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the image\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.cast(img_array / 255.0, tf.float32)  # Normalize to [0, 1]\n",
        "    img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
        "    return img, img_array\n",
        "\n",
        "# Path to the image\n",
        "image_path = '/content/sample_data/pexels-svetozar-milashevich-99573-1490908.jpg'\n",
        "img, img_array = load_and_preprocess_image(image_path)\n",
        "\n",
        "# Make a prediction\n",
        "predictions = model_1.predict(img_array)\n",
        "\n",
        "# Output the prediction\n",
        "class_prediction = (predictions > 0.5).astype(\"int32\")\n",
        "predicted_class = \"Dog\" if class_prediction[0][0] == 1 else \"Cat\"\n",
        "\n",
        "# Display the image and prediction\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Prediction: {predicted_class}\")\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "mSn8r6nVjonc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0pc5CRSQkK5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcZsl6YFj_hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWlg7Salj9_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CanLGwf7jw77"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}